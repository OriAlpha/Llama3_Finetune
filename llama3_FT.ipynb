{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install tensorboard transformers==4.40.0 accelerate==0.29.3 evaluate==0.4.1 bitsandbytes==0.43.1 huggingface_hub==0.22.2 trl==0.8.6 peft==0.10.0 chardet\n",
        "!pip install torch==2.2.2 torchvision==0.17.2 torchaudio==2.2.2 --index-url https://download.pytorch.org/whl/cu118"
      ],
      "metadata": {
        "id": "mo0gqX0XkeJs"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install gensim nltk datasets==2.18.0 keybert"
      ],
      "metadata": {
        "id": "-ia9Zof8kXNE"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "#!pip install flash-attn --no-build-isolation"
      ],
      "metadata": {
        "id": "i-2a_FaHST2k"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Usage of KeyBERT model to extract keywords from the abstracts of machine learning research papers available on the ArXiv dataset.\n",
        "\n",
        "* **Load Dataset**: Load the \"CShorten/ML-ArXiv-Papers\" dataset from the Hugging Face Hub, specifically the training split.\n",
        "* **Extract Abstracts**: Extract the abstracts of the papers from the dataset to use for keyword extraction.\n",
        "* **Initialize KeyBERT Model**: Initialize the KeyBERT model, which leverages BERT embeddings for keyword extraction.\n",
        "* **Extract Keywords**: For each abstract, extract keywords using KeyBERT, considering only single words (unigrams) and removing common English stop words. Collect these keywords into a list.\n",
        "* **Store Results**: Store each abstract along with its associated keywords in a list."
      ],
      "metadata": {
        "id": "3IH3gp026ykk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keybert import KeyBERT\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Load the dataset\n",
        "dataset = load_dataset(\"CShorten/ML-ArXiv-Papers\")[\"train\"]\n",
        "# Only use 500 samples for faster training, but dataset has more than 30K.\n",
        "dataset = dataset.shuffle(seed=65).select(range(500))\n",
        "\n",
        "# Extract abstracts to train on\n",
        "abstracts = dataset[\"abstract\"]\n",
        "\n",
        "# Initialize KeyBERT model\n",
        "kw_model = KeyBERT()\n",
        "\n",
        "# Extract keywords and associate them with abstracts\n",
        "keywords_labels = []\n",
        "for abstract in abstracts:\n",
        "    tmp = kw_model.extract_keywords(abstract, keyphrase_ngram_range=(1, 1), stop_words='english')\n",
        "    labels = [keyword for keyword, score in tmp]\n",
        "    labels = ', '.join(labels)\n",
        "    keywords_labels.append([abstract, labels])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 624,
          "referenced_widgets": [
            "04c83881e1cf4405b419c2a4b2d6c9b0",
            "6d864116e9234f8ba6033a8960ce6d29",
            "cb6623a106bb4c6d8b3589ca3a0bbf03",
            "214bb04593ed4d78a607e66f01c25deb",
            "5196c381f6094a448ab0561c5baa517b",
            "d84636af9eb5423285add3192ba7dbbb",
            "c1edf2fd058a4b2799a21d525544405c",
            "3c31924b183e48bba793c8ec43b60f75",
            "76c46ca5ac0b48039878be857e2ca59f",
            "9d795f45eaeb48119da572e66749282d",
            "1bd86664543c42fdbd81d4e6ac5b3c70",
            "066c70a5f54d4b4ea9c48ddfb7669973",
            "e4dd4ae1fda34d6c8db06d20783f2cba",
            "930ca102cb254df0bd69e693efbb6535",
            "7234d9f57a0646d2acb6b8614cabf1dd",
            "add9d7972dd1416ab5de4116d27a2896",
            "b7923ca8c9a848a2a927be2a6c4d319b",
            "1ff92e958d594db8b42dfd4dea27652f",
            "17558cac1f6d46938812f8c3de2452d0",
            "da208c36d5824369a6992142776c88ac",
            "d4cd4a32f1e64a2ea0f680acd07f8501",
            "655d9032640b4309b227e0148bd44797",
            "3e72e6faf642454888a5758c50b5e56a",
            "d7eb4d846aec4717a02a6d281bcecd9c",
            "a7677bb8bfda45fdba15e51aaf1145e2",
            "8bf73a6670a1470a9802e8850d97a78c",
            "96215ca4c9204c01a92d25905062479e",
            "ab4dc52d05004b43b304636bb43a325f",
            "3460b83495f546deb2b42b81e41de357",
            "34042c6c81ef4025932d92bcc8151a0e",
            "f828eee5a6ff40f3aac3c147aa41a149",
            "0001ba3c146245aeb2020a5179d22662",
            "58384dd45cd04aab8950e7607f0bd774",
            "3c99c80d279d4040b634a1168a0d08d7",
            "7d9b6f340e3c420ca8d8b2125f4c060c",
            "9fd26b10a5de431e87a1e9463cf1cae2",
            "de395459db294d4b84fb22acfd54c21f",
            "242a1ff0e77a4d0989a0967830c3bfdc",
            "9b5b589c65284671a35bde6a86e14853",
            "170eea4b2da34c818c7d808a080672f4",
            "953b0ab671ad413ebba15ffce62b71f4",
            "6b4cf9f8b1564577be4c0379e0d8570f",
            "e50e4385385a405584d1813ed3ed848b",
            "88ec11d5ff9a4b149e825c1bd139161a",
            "204502649cfd4480b324c8ca91c617e9",
            "0932180d29e1469daaa3e4f01c135b17",
            "98ce444ef2ec425ea33dd29cfc5cd397",
            "09a34100e34e4d21894d847ad42911fb",
            "63f1e69eee634d89b882a731a1a28219",
            "5ac2bb6eec3947faade1a181785dba4a",
            "ea366ded1c3c4109a4554c0498a7f87e",
            "36e2fa137eed47a6972a7a4071c9f958",
            "965d7c7eebc442f28a219f3dd4e8c9d5",
            "58a3df8a446d4c43aa0a4e3bbb3a9331",
            "d0cddcfabdb84e5ca8d88924f84e5157",
            "916735f9b53d4633a78eee7e19c3e200",
            "1d862e45a636414e834d4041c0e2a472",
            "47641e92aff24701a61836caa4f93b3d",
            "b83b1e5ae2f443fc9b98cf2d666d2c5a",
            "6789c527a6ca4572beab40ca0cde5ba8",
            "be7dc7a5f27f47d78d2d83fed1e8dda1",
            "ce7e49ba8638408797d51d671d22a8cd",
            "6114aae1297341f2b423f7a960476cb8",
            "0ef79b21a8fd4c96ae5624f5a6a62723",
            "87a9d3ac16ff4b15b278400163df697b",
            "a8957aa174bf4c0a89622d265c602c4b",
            "a607bb6ef3044fe29b32283e32220a84",
            "fafd9fe7cc664105935c7e1f8fb7074d",
            "a0b5f826cbd2455eaf76364fff5846d2",
            "9a3d0defbc5d42789ad50d4be936df67",
            "a1018c6f52b94759abb1253974697f85",
            "466f7667e3644b55b1d38e7be961e62d",
            "761ea156f8144994b9bca80b58ed80e2",
            "1b99cc502f8443b79d5a24a847caf183",
            "d7c914b9d262403ab93b147515339383",
            "2cda8c06e69b4f3097f07af4aa1bd986",
            "762c7b42b0f24277adabc232f9370f22",
            "fcbd3b465374424b88bda1e976e01da9",
            "c166c70f3f804ef4bcb16c1456a61547",
            "a0efda6de11c4a379d1020ed64b88ac2",
            "015ed060f856426a96927b4da885d245",
            "6f089d55db164f1bb26cd2864fe92d22",
            "79bab5b230084eebbfa2bec25a409fd0",
            "c096169f4a394fc2b205202329fc1455",
            "b7477ad57bbc43a7b8bfe23aa3104bb0",
            "97479a9581234eca9c60d6a67b0a5a72",
            "09288da17e124869ae9cd71b44806048",
            "c219472264b6471fbb3c1a999f28476f",
            "69242c01e5a84ca794c61e3a4389fe01",
            "311147f993e74f699c9185ae93140a92",
            "91ff258c86fd434382391e7049f8a76c",
            "4d20c24505ec4163b82dcd2109ac57d8",
            "2d86eb7c216c4f4cab6d43248151d528",
            "cdbcb9f6ccb54334b03df5a0def918c0",
            "7e2be87ec3c5401482efd6cd3c5a0f26",
            "5d4bd6e81f624076b622eebcd1c7000b",
            "7eb06256a8854ed489dac267a6529df2",
            "c1a9fc9387de48f4b38a4830649148a8",
            "6d6fd1c1871e469ca5557e3694713132",
            "d1fea4c9ee2e452c87f38ea1e686106d",
            "7eba6579f5f44a52986567c5242c52f3",
            "e16f6f7518464e42a0bbd60baa5a8512",
            "f650fce0d30d47c8b5e7d7ddd8428fa1",
            "4fe67197ff93450cbdf915db1028f6af",
            "c2c03b36929c40648ee4dfa89fa757cb",
            "ff4109a5e5894092a6436813a4c89ba7",
            "337e9db666c048c4a2dc34077a66b85d",
            "3db60320f69d43d5bac2ad6e38c25005",
            "6ea7ac944e2245769353376362033d1d",
            "b6fbe344c1d746489c58ffa1bca29452",
            "058d593e6cbd4019983c9183c2c7782e",
            "9347fbcb181e4c32a5d9388d7cfa50f4",
            "cd87f2fe0548425f91d28907b4a517fa",
            "7fdf8cfd7a4f4066aa2084e317a63032",
            "61e935285ba04ed395c4973f28584648",
            "0c3306ca4f0b4d4dbde74e0389f543d5",
            "4f9946a2b00a4314b403b07f086c8703",
            "1c736868a2704f7686c91537aa815a8f",
            "5488da45d7fd45debfdeec617ab36d8b",
            "c685c2bcf68b46159c67dee5e4e08cd6",
            "3a6c9fa190d04b2c8398b72bfbe577c9",
            "4ceaabd0a830482098f3fb146ded279c",
            "62bb54613b2843bbb9ac7972e0474ea6",
            "23f94c9170f44fd4b7f24bde4bfe15bc",
            "e7f03aa9196f4f04a1ff1ff4a78fc2a4",
            "6ba5eccd6ade47938793d40a156efe47",
            "a7fa587ef24f4d1fb77bf0cb2dbade97",
            "0ea7ff1ea8f740b3bc846a87ce7e1e95",
            "675f024689d74217b3f50065db11cd85",
            "ba8df299d92445a4b403d4d96999795c",
            "6118912b70de4e5bab097b8cf9856e05",
            "7515a7156bd7437f96ddb3b62b50c4b7",
            "ce69e657cf6941ea9e342ed88bc73ea6",
            "50e936997f744058be494f24f16d29db",
            "d14862bd307e4d858440549076613755",
            "9fa356cd6d674a7cba6fe258914cff3e",
            "c570588790f34f6bb23d7c3b7d6dccc0",
            "27e208e295d6462ea7589ca9fca6edc5",
            "f143fe2e565745c280ce509d1021c803",
            "0745f809b20a4b7b8fe4f4089c21669e",
            "824eeb48968341d9b6a8366e233ecd16",
            "7d2ae4fb47244f5b8884469d8029e512",
            "62c307db7636492b99d1421b077a2127",
            "6894936d8ddc40bca1bb410ac4a56682",
            "57989336c91549c386ebbd7a52393353",
            "a525d24a3f7f4262a4e4cf3a061f40d2",
            "7e0ccd2b3fdc48da92f6b7b9bacb9848",
            "088ba66fe5bb4e09a5741deed005521d",
            "246d8780f9e54d6ab098a2b4cb5f3d3a",
            "ddfcdd7d22684f30a798d0b41de96ad8",
            "2d43445f5c1b44f49d0c46ea977a09ff",
            "ffd80512f9014371b349640cbc0f6cac",
            "f6082b39f15f4a3191303da9dc980131",
            "0ba3559410f9487c998ce2dfe848af6b"
          ]
        },
        "id": "ccvGimck_L-8",
        "outputId": "41280d9b-54d9-475c-feb2-513f4ff40456"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
            "  from tqdm.autonotebook import tqdm, trange\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading readme:   0%|          | 0.00/986 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "04c83881e1cf4405b419c2a4b2d6c9b0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading data:   0%|          | 0.00/147M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "066c70a5f54d4b4ea9c48ddfb7669973"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split: 0 examples [00:00, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3e72e6faf642454888a5758c50b5e56a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3c99c80d279d4040b634a1168a0d08d7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "204502649cfd4480b324c8ca91c617e9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md:   0%|          | 0.00/10.7k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "916735f9b53d4633a78eee7e19c3e200"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a607bb6ef3044fe29b32283e32220a84"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fcbd3b465374424b88bda1e976e01da9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "69242c01e5a84ca794c61e3a4389fe01"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d1fea4c9ee2e452c87f38ea1e686106d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "058d593e6cbd4019983c9183c2c7782e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4ceaabd0a830482098f3fb146ded279c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ce69e657cf6941ea9e342ed88bc73ea6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6894936d8ddc40bca1bb410ac4a56682"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Logging into Hugging Face Hub\n",
        "\n",
        "The `notebook_login` function from the `huggingface_hub` library is used to authenticate and log into the Hugging Face Hub. Also request access for llama3 model since its closed model, permission can take up to 2 min to 2 hrs.\n",
        "* Request llama: https://huggingface.co/meta-llama/Meta-Llama-3-8B\n",
        "* Create HF token, if doesnt exist: https://huggingface.co/settings/tokens"
      ],
      "metadata": {
        "id": "EANiRcgEv_Vt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145,
          "referenced_widgets": [
            "68ba4ce1d0ef4055ad1513bbd367083e",
            "f35ed766eee548238836c18744999b59",
            "d21548d8022240508d9e7490a72f371c",
            "84f57c82696a4ea490eb99cd7d66696f",
            "ee1dae3f94e6437d8c74ca5969059b70",
            "6e0aa021a20f4091bb1d898c96da0fc2",
            "9e429ce573a04038a88759a358baf172",
            "851cef063e984d98ad01879174fea7e9",
            "23f93a507ced49d5a98c5e35a32dc695",
            "f8b4188c8524439c9f358ab81373d5c3",
            "2760addf1ce145ccad2334761082c90e",
            "e830dee49d224d13996f6180897e2c62",
            "48357263e83e443aaf1a435a66a5472c",
            "a7aa21f25e32438da9455b2cda88c270",
            "032129559ead489b8cbc975547c43ef2",
            "15c9b01634cc49228c7014118d13585f",
            "74a7203cf5c146bd936870b750c02162",
            "85b9a7268f6a48a3bc2e14dc4f02202a",
            "21eeb8128c434601bccf3fcae0e1328e",
            "b89ca51c7c5b48ea80d01b4ca47666cb",
            "79a4642ee8f74aa5aacbc23e6a5e6ab2",
            "f03cb9360ebc49b984ca55ee00d7671c",
            "aaea9c07480140dfbe6973b08bdd1f03",
            "fc192d96a8a24cbf9c273217578ed6ce",
            "e5718334f80c44f994ec74c412a078dc",
            "bfedf04d3cee4e4d82dfdd52d1f5b6ee",
            "87bc0746f28d45f1986ed8b1313cd9a7",
            "462c577b15494216b0fb982ffe8df899",
            "e636c40d5b9b4433a6f089fd375afc51",
            "5b84108889264b108ae13958707e4d91",
            "ded58cedadf849c3a9532080fe7c05b7",
            "fb3e008b627f4273b35ed1b0c54365cc"
          ]
        },
        "id": "mU1LHiS-QwsR",
        "outputId": "f1b516ce-a536-4155-e06a-106af16eeb8d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv\u2026"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "68ba4ce1d0ef4055ad1513bbd367083e"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing Libraries for Model Training and Fine-Tuning\n",
        "\n",
        "   - `os`: For interacting with the operating system.\n",
        "   - `random`: For generating random numbers, useful for reproducibility.\n",
        "   - `torch`: The core PyTorch library for tensor operations and deep learning.\n",
        "   - `AutoModelForCausalLM`: For loading pre-trained causal language models.\n",
        "   - `AutoTokenizer`: For loading tokenizers compatible with the models.\n",
        "   - `TrainingArguments`: For specifying training configurations.\n",
        "   - `set_seed`: For setting the random seed to ensure reproducibility.\n",
        "   - `EarlyStoppingCallback`: For implementing early stopping during training.\n",
        "   - `BitsAndBytesConfig`: For configurations related to memory optimization and quantization.\n",
        "   - `SFTTrainer`: For supervised fine-tuning of transformer models.\n",
        "   - `setup_chat_format`: For setting up the chat format, if applicable.\n",
        "   - `LoraConfig`: Configuration class for Low-Rank Adaptation (LoRA).\n",
        "   - `AutoPeftModelForCausalLM`: For loading pre-trained models with PEFT configurations.\n",
        "\n"
      ],
      "metadata": {
        "id": "7xG-rwoH7E1R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import torch\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    TrainingArguments,\n",
        "    set_seed,\n",
        "    EarlyStoppingCallback,\n",
        "    BitsAndBytesConfig\n",
        ")\n",
        "from trl import SFTTrainer, setup_chat_format\n",
        "from peft import LoraConfig, AutoPeftModelForCausalLM"
      ],
      "metadata": {
        "id": "OUWiZxNAOYy_"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuration parameters\n",
        "model_id = \"meta-llama/Meta-Llama-3-8B\"\n",
        "torch_dtype = torch.bfloat16\n",
        "quant_storage_dtype = torch.bfloat16\n",
        "gradient_checkpointing = True\n",
        "\n",
        "# Setting a seed for reproducibility\n",
        "set_seed(42)"
      ],
      "metadata": {
        "id": "G7gNsXk4OedE"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the tokenizer\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    TrainingArguments,\n",
        "    set_seed,\n",
        "    EarlyStoppingCallback,\n",
        "    BitsAndBytesConfig\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130,
          "referenced_widgets": [
            "7f1cd41c0e10497194e085f17548377e",
            "4f5ae3d989c94fbf8165c1d3de9400f9",
            "c25fbbe89bc249c9bc68031f740aab44",
            "bc25cb70d818475a82cdacf8183d02cd",
            "80b552f453ae4265984e54218442e100",
            "62f04ac26a794e2eba53aee6a5fbca01",
            "cc2e310a9c5f468bbac9392f8b18f2a3",
            "8de3cd6770ae40379565ee6ced129639",
            "545d6d3e26854cd989717fb266334ee0",
            "9287ef21aecf47d09906fe9811bc5ade",
            "814eeb8de74440538242a920bfd6041f",
            "35f2ce40186847a8b8670c0ceb5c456a",
            "444f28c81bb54a0c92c98aa5b35ce770",
            "3f2f19d5ec4b4fa3a4b6eda0a90f1500",
            "f3567f79ec5f4a22bd8fffc6280e68a7",
            "ba4bf41b4ec0403d957bfa0502b51d12",
            "988b8ac6af4b428c8e2a600d8c2dc942",
            "cac67d0e375647cc9adcad4a90932717",
            "3bebc0938784491fbe92865d14deb250",
            "fc8a7d17297546708e8bd555eeb7ab11",
            "8de78a9abfe1457cb41617f0e88f2352",
            "c80df51311164a738f1b4bb111833cea",
            "4bd3d9c6d0d6400eb51a025c24dd2813",
            "55f72429120a4feb9d70ff77a50689ac",
            "7dbb4b9b3cad4d7aa410727b4589dc47",
            "d2570c3129f949e69c265d0474b20501",
            "17d6958ba315404ebf13d9f0159094da",
            "6077452bde674d37afebe3bcc657f932",
            "b2de637fbbc14bdfbdd0c85da90206a8",
            "68b47b56a777454280ffcccf16218d67",
            "2887a87772ee45ff9e1f216fb85adc78",
            "b3b4faad958f448492fcc84bbecb8b14",
            "059934b59616491c9ed081ce84724937"
          ]
        },
        "id": "cdGyLYUKOb3F",
        "outputId": "319a87a3-11f6-4d1c-cb96-aaca9d66fc81"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/50.6k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7f1cd41c0e10497194e085f17548377e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "35f2ce40186847a8b8670c0ceb5c456a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/73.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4bd3d9c6d0d6400eb51a025c24dd2813"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "x3RxynwMdhWg"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Custom Prompt Formatting for llama Model\n",
        "\n",
        "To prepare the prompts for the llama model, we define a custom function `formatting_prompts_func`. This function takes a list of examples, where each example consists of an input text and its corresponding response. The function formats each example into a structured prompt that includes the input text, the expected response, and an End-of-Sequence (EOS) token to denote the end of the prompt.\n",
        "\n",
        "The provided `llama_prompt` template defines the structure of the prompt, with placeholders for the input and response texts. It ensures that the assistant's role and standards are clearly stated in each prompt.\n",
        "\n",
        "The `EOS_TOKEN` variable is set to the tokenizer's end-of-sequence token, ensuring that each prompt has a proper ending to prevent infinite generation.\n",
        "\n",
        "Finally, the `formatting_prompts_func` function iterates over the examples, formats each one according to the `llama_prompt` template, and appends the EOS token. The formatted prompts are then returned as a dictionary with the key \"text\".\n"
      ],
      "metadata": {
        "id": "pBvLkHGR7Ql4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llama_prompt = \"\"\"As an assistant for topic modeling, my role is to act considerately, reliably, and as a trustworthy guide in the domain of topic modeling. I aim to provide assistance that meets these standards consistently.\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\"\"\"\n",
        "\n",
        "# Must add EOS_TOKEN\n",
        "EOS_TOKEN = tokenizer.eos_token\n",
        "def formatting_prompts_func(examples):\n",
        "    texts = []\n",
        "    for example in examples:\n",
        "        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
        "        text = llama_prompt.format(example[0], example[1]) + EOS_TOKEN\n",
        "        texts.append(text)\n",
        "    return { \"text\" : texts, }\n",
        "pass\n",
        "\n",
        "\n",
        "dataset = formatting_prompts_func(keywords_labels)"
      ],
      "metadata": {
        "id": "4tU4KzaxNZM6"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Configuration for Model Quantization\n",
        "\n",
        "Snippet below shows the configuration for quantizing a language model using the `BitsAndBytesConfig` class. Quantization is a technique used to reduce the precision of the model's weights and activations, thereby decreasing memory usage and improving inference speed, particularly on hardware with limited computational resources.\n",
        "\n",
        "The `quantization_config` object is initialized with the following parameters:\n",
        "- `load_in_4bit`: Enables loading the model weights in 4-bit format.\n",
        "- `bnb_4bit_use_double_quant`: Indicates whether to use double quantization for 4-bit weights.\n",
        "- `bnb_4bit_quant_type`: Specifies the quantization type, with \"nf4\" representing non-fully quantized 4-bit quantization.\n",
        "- `bnb_4bit_compute_dtype`: Defines the data type for computing quantization parameters, typically `torch_dtype`.\n",
        "- `bnb_4bit_quant_storage_dtype`: Specifies the data type for storing quantized weights.\n",
        "\n",
        "Next, the model is loaded using the specified `quantization_config`. Additional parameters include:\n",
        "- `model_id`: The identifier or name of the pre-trained model to load.\n",
        "- `attn_implementation`: Specifies the attention mechanism implementation, with \"sdpa\" representing scaled dot-product attention.\n",
        "- `torch_dtype`: The Torch data type used for computation, matching the specified `quant_storage_dtype`.\n",
        "- `use_cache`: Controls whether to use caching during inference, typically set to `True` unless using gradient checkpointing.\n",
        "\n",
        "After loading the model, it is set to evaluation mode using `model.eval()` to ensure that parameters are fixed during inference.\n"
      ],
      "metadata": {
        "id": "KFll1A767bwo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuration for model quantization\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch_dtype,\n",
        "    bnb_4bit_quant_storage_dtype=quant_storage_dtype,\n",
        ")\n",
        "\n",
        "# Load the model with the quantization configuration\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    quantization_config=quantization_config,\n",
        "     attn_implementation=\"sdpa\", # use sdpa, alternatively use \"flash_attention_2\"\n",
        "    torch_dtype=quant_storage_dtype,\n",
        "    use_cache=not gradient_checkpointing,\n",
        ")\n",
        "\n",
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 791,
          "referenced_widgets": [
            "fef57991c1734a0caac4a37e147b6ecc",
            "3b2d779bc2d249ceb03934e0a601e14a",
            "4aa0f71044414129a4dbe36b5a02773c",
            "91bbf8dc59a641f092b48af5384d3181",
            "81a99e9d11824eeeb106bec378a6471d",
            "397f04f078db4c92b60f6985f18d89e9",
            "1030e08b91ea4c7285cdded618379713",
            "4db0e24078ec440192c08e036e310239",
            "bda088e6f38c44d482024abf8e6a485e",
            "50e88ce8359a4523973a4c97f03e885d",
            "fdd083a49a184cabba0076ae27f9e847",
            "a7dfb7a989a6452d80fae94ef1a401c9",
            "7104300007c64c56a42383c13d7eac8d",
            "3d5c8036e5a54f4bb3499c4e33ee1087",
            "5161efaeb9b748d49fad305fdad3772e",
            "e0cbaee6dcfe4aa4b0e3a9c4148ec45d",
            "e08bfc34cf0b4feb85a2bb218bb4d37e",
            "f5a12f96ebd24b8a8e468eb5a8bd82d1",
            "4814b884bd914ec59ae4873cebdbea99",
            "f8077af28bea44a1a4ae8c46bca089cd",
            "d663f95263a64a758268e4f6df1825a7",
            "92eff2f4cb8047f29083b4a10c1fda2b",
            "15fd230273264a60a3d33b516c4039d6",
            "c71084882e394d128e19f7e3860e2511",
            "86ca2d69e9d34a8b848a28d9bbe91e9c",
            "928f395c23824ea5a478b3542d08a427",
            "96259b2dfa0442d38018c688fe8ce632",
            "aefaa529b2684a269fa8da326cab90a4",
            "e21ee4f0f8674ea7b579dde6231d0862",
            "0110aa9a707f4c8785babf695c685e07",
            "046068f054204607a76132a2297c9be2",
            "6e2e3295129a4416900730feb6d824d1",
            "ca1cb47339334aab9eab55141c81d387",
            "2e28a4d0fa7a4f218b98fdf04b3e3f51",
            "52f3f2f02dcc45e08ead57e6a2ecb3c1",
            "f02a3146296f4e7ca03f5d40dfb00b45",
            "3a1d48c1d56941b1a355c10de9453a85",
            "857fc36f36d14d328931849fce29dc1d",
            "db8b8ca0c0ff4ac98f0caf0690f54519",
            "73e7908e6df74f48a7ea5a9e09b13d20",
            "aee2ef232d6644b7b01dba905de388ab",
            "c1eaed6aed0d47fc996de85ac1f73917",
            "6fe64ca3474544ed9ec224cb4efa3d0c",
            "c8259ae212ba4ed7b54ed08e75f57676",
            "62046935dc6f4889a6f4239eeb102588",
            "c1fb35e21d954e2baaba8eac4551112c",
            "68a48519d32d4df8adf44a4d56ce7be5",
            "3f28cc9e95ad4ccb9796a47d55765b18",
            "fc7a0eb6db6c4b71a22f5046fccb6cba",
            "935584333bf348ca876a08fa71dcddcb",
            "0ac319dfc6e14b60b8c37d8d00d8986f",
            "d34167de27ac48c383c2f2c7bc357726",
            "fa27ebb62c544730bcedb23af503e645",
            "42664c5e68da46789e38247f7d963a53",
            "dd53297840a14f27a8c558e8edeee171",
            "e630f7b1fb43406ab5d1143998bfda5c",
            "0f99e72350bb4c4b98bad4dac91e4058",
            "9fb65009fa144a0ea00a4f22692e099a",
            "0bbd31b6bed64616b84f067c6e88da53",
            "fc528ee58f42455b9f843c645207d08d",
            "081fb7513fd04f4db5ccef13cb12b5fa",
            "8c145e20102b416d9cddcaae3d678328",
            "64ef4c8700f741ebb5da11e696b64052",
            "ead3bceade8a4593ba0a43802d90f639",
            "0218ec64c99e467bbe36d51aa817e249",
            "b9627331e7f84a89aa809d42a9a25988",
            "f364a9b62c004dc4a527f62c8acd47f9",
            "538172fefa8e4ea2bf9e92d64e87cf36",
            "a76c903855734206bf52a1df15ef7ab9",
            "23e150474d7d4565854010299683d045",
            "c8b9d2ba1d2c482394f3a515d02e1c51",
            "e93e873d3962438db4161b28140d2bab",
            "bca6e61e331a4885ae3277c0376ea25c",
            "2479d16eeff840d28fe86d7d5679e334",
            "2f368428436042a09f6c4d4bb2340b3f",
            "c48f769c7c3440a788acfe5425b30c5a",
            "1bc2b4a3453747c1a7ec58bd262cabf0",
            "b1d8a1fe4bce48418437ee2161db5546",
            "ad605b75757b40ff8402817e86152107",
            "38f533f0d43b4b0185eaa746dcbb170b",
            "b91e164eff1d4b4fa2c1d7bb2a27f27e",
            "f4bc3e27cac74142981dc9f76c7fd8cf",
            "756364918ce047cf96d0849356f09bd3",
            "a05ca798fa3543c6b31714c59f7c4dc2",
            "6aa019cc31ca45b18d1833273471dc58",
            "7e12baf5ace94eaca0f7d9dc5252741e",
            "e63bb9fc51984d2ab227a7e2cdf2c9f5",
            "b38c5e7820484199add4ea077ef197f7",
            "f8ac26c517bd408284efaec72a8114e1",
            "07b90259072b40bb91909f8d4ab13329",
            "32a0147f98f34fc781dcc46bd8b0a6dc",
            "309f78b9867349d88a9cd53f9bb9efc0",
            "b5a7dec3395146d69fd496abe27cd038",
            "52f9e78225114e59ab04ba3a929122ae",
            "281bf9174ccd48cbb1374570651d5a93",
            "158eb7bce7d04a1e9bd97f1d57be74b1",
            "44c3cf44c0204aa7acaec0cd29cf7ecc",
            "862968430d794e59910ebb43c5a1d1f5",
            "59466e6a78184b5aa9919224f5bf3c21"
          ]
        },
        "id": "H53QfqGvOvnn",
        "outputId": "1ec62cb2-7f57-4403-cbf5-efd5442f8a60"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unused kwargs: ['bnb_4bit_quant_storage_dtype']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/654 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fef57991c1734a0caac4a37e147b6ecc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a7dfb7a989a6452d80fae94ef1a401c9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "15fd230273264a60a3d33b516c4039d6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2e28a4d0fa7a4f218b98fdf04b3e3f51"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "62046935dc6f4889a6f4239eeb102588"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e630f7b1fb43406ab5d1143998bfda5c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f364a9b62c004dc4a527f62c8acd47f9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b1d8a1fe4bce48418437ee2161db5546"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/177 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f8ac26c517bd408284efaec72a8114e1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LlamaForCausalLM(\n",
              "  (model): LlamaModel(\n",
              "    (embed_tokens): Embedding(128256, 4096)\n",
              "    (layers): ModuleList(\n",
              "      (0-31): 32 x LlamaDecoderLayer(\n",
              "        (self_attn): LlamaSdpaAttention(\n",
              "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
              "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
              "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (rotary_emb): LlamaRotaryEmbedding()\n",
              "        )\n",
              "        (mlp): LlamaMLP(\n",
              "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
              "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
              "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
              "          (act_fn): SiLU()\n",
              "        )\n",
              "        (input_layernorm): LlamaRMSNorm()\n",
              "        (post_attention_layernorm): LlamaRMSNorm()\n",
              "      )\n",
              "    )\n",
              "    (norm): LlamaRMSNorm()\n",
              "  )\n",
              "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Enabling Gradient Checkpointing and Configuring LoRA (Low-Rank Adaptation)\n",
        "\n",
        "Code below checks if gradient checkpointing is specified and enables it if required. Additionally, it sets up the configuration for Low-Rank Adaptation (LoRA).\n",
        "\n",
        "## Gradient Checkpointing\n",
        "Gradient checkpointing is a memory optimization technique used during backpropagation to reduce the memory footprint by trading off computation for memory. If `gradient_checkpointing` is specified, the following steps are performed:\n",
        "\n",
        "- The `model.gradient_checkpointing_enable()` method is called to enable gradient checkpointing for the model.\n",
        "- The `model.enable_input_require_grads()` method is called to ensure that input gradients are enabled, which is necessary for gradient checkpointing to work effectively.\n",
        "\n",
        "## Configuration for LoRA\n",
        "LoRA is a technique used for low-rank adaptation, which helps in efficient fine-tuning of large language models. The `peft_config` object is initialized with the following parameters:\n",
        "\n",
        "- `lora_alpha`: Hyperparameter controlling the strength of the low-rank adaptation, typically set to a value around 8.\n",
        "- `lora_dropout`: Dropout probability used in the LoRA layer to prevent overfitting.\n",
        "- `r`: Rank of the low-rank approximation, specifying the number of singular values retained in the factorized weight matrix.\n",
        "- `bias`: Specifies whether biases are included in the low-rank approximation, set to \"none\" here.\n",
        "- `target_modules`: Indicates which modules are targeted for low-rank adaptation, set to \"all-linear\" to target all linear layers.\n",
        "- `task_type`: Specifies the type of task, here set to \"CAUSAL_LM\" for causal language modeling.\n",
        "\n",
        "These configurations set up the model for efficient fine-tuning using LoRA, enhancing its performance while maintaining memory efficiency.\n"
      ],
      "metadata": {
        "id": "hzFTwFvH70EE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Enable gradient checkpointing if specified\n",
        "if gradient_checkpointing:\n",
        "    model.gradient_checkpointing_enable()\n",
        "    model.enable_input_require_grads()\n",
        "\n",
        "# Configuration for LoRA (Low-Rank Adaptation)\n",
        "peft_config = LoraConfig(\n",
        "    lora_alpha=8,\n",
        "    lora_dropout=0.05,\n",
        "    r=16,\n",
        "    bias=\"none\",\n",
        "    target_modules=\"all-linear\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")"
      ],
      "metadata": {
        "id": "pY0zYd_2OyJv"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup for Training\n",
        "\n",
        "Early stopping is a technique used to halt the training process when certain conditions are met, typically to prevent overfitting or when the model's performance plateaus. In this code snippet, we configure early stopping for the model training process.\n",
        "\n",
        "The `early_stop` object is initialized with the following parameters:\n",
        "- `early_stopping_patience`: Specifies the number of epochs with no improvement after which training will be stopped.\n",
        "- `early_stopping_threshold`: Defines the threshold for measuring improvement, indicating the minimum improvement required to continue training.\n",
        "\n",
        "The `trainer` is then initialized with the following configurations:\n",
        "- `model`: The pre-trained language model to be fine-tuned.\n",
        "- `tokenizer`: The tokenizer associated with the model.\n",
        "- `train_dataset`: The training dataset, converted from a Pandas DataFrame.\n",
        "- `dataset_text_field`: The name of the text field in the dataset.\n",
        "- `peft_config`: Configuration for Parameter-Efficient Fine-Tuning (PEFT).\n",
        "- `max_seq_length`: The maximum sequence length for tokenized inputs.\n",
        "- `packing`: Specifies whether to use packing for memory optimization, set to `False` here.\n",
        "- `dataset_kwargs`: Additional keyword arguments for dataset processing.\n",
        "- `args`: TrainingArguments object specifying various training configurations such as number of epochs, batch size, learning rate, etc.\n",
        "- `output_dir`: The directory to save the trained model and logs.\n",
        "- `report_to`: Indicates where to report training metrics, here set to \"tensorboard\".\n"
      ],
      "metadata": {
        "id": "_gzb4ihU7hmO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup for early stopping, if needed\n",
        "early_stop = EarlyStoppingCallback(early_stopping_patience=5, early_stopping_threshold=0.3)\n",
        "\n",
        "# Initialize the trainer with all configurations\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=Dataset.from_pandas(pd.DataFrame(dataset)),\n",
        "    dataset_text_field=\"text\",\n",
        "    peft_config=peft_config,\n",
        "    max_seq_length=512,\n",
        "    packing = False,\n",
        "    dataset_kwargs={\n",
        "        \"add_special_tokens\": False,\n",
        "        \"append_concat_token\": False,\n",
        "    },\n",
        "    args = TrainingArguments(\n",
        "    num_train_epochs = 2,\n",
        "    per_device_train_batch_size = 2,\n",
        "    gradient_accumulation_steps = 4,\n",
        "    warmup_steps = 5,\n",
        "    warmup_ratio = 0.03,\n",
        "    learning_rate = 2e-4,\n",
        "    fp16 = not torch.cuda.is_bf16_supported(),\n",
        "    bf16 = torch.cuda.is_bf16_supported(),\n",
        "    logging_steps = 1,\n",
        "    optim = \"adamw_torch_fused\",\n",
        "    weight_decay = 0.01,\n",
        "    lr_scheduler_type = \"linear\",\n",
        "    seed = 3407,\n",
        "    output_dir = \"adapter_model_dir\",\n",
        "    report_to=\"tensorboard\"\n",
        "    )\n",
        "    # callbacks=[early_stop],\n",
        ")"
      ],
      "metadata": {
        "id": "lemjbjBDOzsD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "de7f3fb2d60f473fabbb61e3dd81818a",
            "1ea613fc103243e18c3af58305693239",
            "e260ee710689454c9b4e22825b929469",
            "1a9054be1869401e9ed8636bb34bd7b3",
            "a0489641352c47e0a490c9f65ae767e0",
            "6f89d61aa7a640e1b3136b632eb75f9d",
            "69bc559986d44ea89419f848624ea035",
            "ffb804831f174fb5bc13533e64130c3d",
            "29cd3ebaaedb45fdb3a1bb453d0f797b",
            "7033c183ae4f44f3a41cb23d469e314e",
            "7f42e21070be4bd0a93f491b7eaa0dc2"
          ]
        },
        "outputId": "6150f6c4-04a1-48ef-ddb7-c4a9ac57cdc7"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "de7f3fb2d60f473fabbb61e3dd81818a"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print trainable parameters if this is the main process\n",
        "if trainer.is_world_process_zero():\n",
        "    trainer.model.print_trainable_parameters()\n",
        "\n",
        "# Start training\n",
        "trainer.train()\n",
        "\n",
        "# Save the trained model\n",
        "trainer.save_model()"
      ],
      "metadata": {
        "id": "2cIkRvspO5c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c86d1284-d7d7-4266-8b08-a2846aedf348"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 41,943,040 || all params: 8,072,204,288 || trainable%: 0.5195983464188562\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='124' max='124' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [124/124 30:26, Epoch 1/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.581200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.759100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.669600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>1.575900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>1.712500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>1.647700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>1.726500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>1.607200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>1.665900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>1.627200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>1.770500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>1.717800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>1.683600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>1.548000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>1.551300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>1.534300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>1.624900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>1.624800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>1.525700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>1.665100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>1.673800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>1.701300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>1.569800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>1.768800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>1.680000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>1.716000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>1.674500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>1.629900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>1.615500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>1.748600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31</td>\n",
              "      <td>1.515700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>1.565000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33</td>\n",
              "      <td>1.671800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34</td>\n",
              "      <td>1.568800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>1.598400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>1.650700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37</td>\n",
              "      <td>1.723500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38</td>\n",
              "      <td>1.706400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39</td>\n",
              "      <td>1.696700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>1.726400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41</td>\n",
              "      <td>1.744100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42</td>\n",
              "      <td>1.596100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>43</td>\n",
              "      <td>1.719300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44</td>\n",
              "      <td>1.540400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45</td>\n",
              "      <td>1.707100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>46</td>\n",
              "      <td>1.750300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>47</td>\n",
              "      <td>1.693300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48</td>\n",
              "      <td>1.627100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>49</td>\n",
              "      <td>1.815100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>1.493700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>51</td>\n",
              "      <td>1.626200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>52</td>\n",
              "      <td>1.786000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>53</td>\n",
              "      <td>1.670200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>54</td>\n",
              "      <td>1.732700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>55</td>\n",
              "      <td>1.662500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56</td>\n",
              "      <td>1.620900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>57</td>\n",
              "      <td>1.818700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>58</td>\n",
              "      <td>1.715800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>59</td>\n",
              "      <td>1.648200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>1.880300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>61</td>\n",
              "      <td>1.643400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>62</td>\n",
              "      <td>1.763400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>63</td>\n",
              "      <td>1.658400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>64</td>\n",
              "      <td>1.553100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>65</td>\n",
              "      <td>1.537700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>66</td>\n",
              "      <td>1.623400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>67</td>\n",
              "      <td>1.510300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>68</td>\n",
              "      <td>1.401700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>69</td>\n",
              "      <td>1.372500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>1.437700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>71</td>\n",
              "      <td>1.280400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>72</td>\n",
              "      <td>1.600300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>73</td>\n",
              "      <td>1.535900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>74</td>\n",
              "      <td>1.414400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>75</td>\n",
              "      <td>1.390000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>76</td>\n",
              "      <td>1.494100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>77</td>\n",
              "      <td>1.337700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>78</td>\n",
              "      <td>1.348000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>79</td>\n",
              "      <td>1.432900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>1.405600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>81</td>\n",
              "      <td>1.355100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>82</td>\n",
              "      <td>1.384300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>83</td>\n",
              "      <td>1.414900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>84</td>\n",
              "      <td>1.432700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>85</td>\n",
              "      <td>1.436000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>86</td>\n",
              "      <td>1.429500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>87</td>\n",
              "      <td>1.394400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>88</td>\n",
              "      <td>1.394300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>89</td>\n",
              "      <td>1.334000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>1.345900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>91</td>\n",
              "      <td>1.354700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>92</td>\n",
              "      <td>1.384300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>93</td>\n",
              "      <td>1.322400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>94</td>\n",
              "      <td>1.364600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>95</td>\n",
              "      <td>1.293700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>96</td>\n",
              "      <td>1.430200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>97</td>\n",
              "      <td>1.568000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>98</td>\n",
              "      <td>1.339400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>99</td>\n",
              "      <td>1.342500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>1.482900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>101</td>\n",
              "      <td>1.477300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>102</td>\n",
              "      <td>1.326200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>103</td>\n",
              "      <td>1.321000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>104</td>\n",
              "      <td>1.341700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>105</td>\n",
              "      <td>1.317500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>106</td>\n",
              "      <td>1.215500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>107</td>\n",
              "      <td>1.417100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>108</td>\n",
              "      <td>1.355100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>109</td>\n",
              "      <td>1.496100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>1.289100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>111</td>\n",
              "      <td>1.257400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>112</td>\n",
              "      <td>1.477700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>113</td>\n",
              "      <td>1.450700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>114</td>\n",
              "      <td>1.323400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>115</td>\n",
              "      <td>1.474500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>116</td>\n",
              "      <td>1.406000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>117</td>\n",
              "      <td>1.394900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>118</td>\n",
              "      <td>1.555800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>119</td>\n",
              "      <td>1.188700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>1.361500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>121</td>\n",
              "      <td>1.298500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>122</td>\n",
              "      <td>1.350200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>123</td>\n",
              "      <td>1.416400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>124</td>\n",
              "      <td>1.194600</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Default value for topic\n",
        "default_topic = \"Supervised learning is the machine learning task of learning a function that maps an input to an output based on example input-output pairs. It infers a function from labeled training data consisting of a set of training examples. In supervised learning, each example is a pair consisting of an input object (typically a vector) and a desired output value (also called the supervisory signal). A supervised learning algorithm analyzes the training data and produces an inferred function, which can be used for mapping new examples. An optimal scenario will allow for the algorithm to correctly determine the class labels for unseen instances. This requires the learning algorithm to generalize from the training data to unseen situations in a reasonable way (see inductive bias).\"\n",
        "\n",
        "# Prompt the user to enter a value\n",
        "user_input = input(\"Please enter the value for src (press Enter to use the default value): \")\n",
        "\n",
        "# Use the default value if the input is empty\n",
        "topic = user_input if user_input else default_topic\n",
        "\n",
        "# Print the value of src\n",
        "print(\"Input:\", topic)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "61S8iDgr41bq",
        "outputId": "e2622251-3dc2-428f-bf2b-ab7a09eb4986"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please enter the value for src (press Enter to use the default value): \n",
            "Input: Supervised learning is the machine learning task of learning a function that maps an input to an output based on example input-output pairs. It infers a function from labeled training data consisting of a set of training examples. In supervised learning, each example is a pair consisting of an input object (typically a vector) and a desired output value (also called the supervisory signal). A supervised learning algorithm analyzes the training data and produces an inferred function, which can be used for mapping new examples. An optimal scenario will allow for the algorithm to correctly determine the class labels for unseen instances. This requires the learning algorithm to generalize from the training data to unseen situations in a reasonable way (see inductive bias).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tokenizer(\n",
        "[\n",
        "    llama_prompt.format(\n",
        "        topic, # input\n",
        "        \"\",\n",
        "    )\n",
        "], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "outputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)\n",
        "\n",
        "text = tokenizer.batch_decode(outputs)\n",
        "\n",
        "\n",
        "# Some postprocessing for output, before displaying\n",
        "response = text[0].split(\"### Response:\")[1].strip()\n",
        "unique_words = list(dict.fromkeys(response.split(', ')))\n",
        "cleaned_response = ', '.join(unique_words)"
      ],
      "metadata": {
        "id": "Jv9HZJWxoEPu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e74db908-2c21-4d2e-b933-2ca7e1745f5e"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Topic:{topic}\")\n",
        "print(f\"Model Response:{cleaned_response}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "btYohaVM5B12",
        "outputId": "9452988b-cfe1-4e81-c696-681602cc4cc5"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Topic:Supervised learning is the machine learning task of learning a function that maps an input to an output based on example input-output pairs. It infers a function from labeled training data consisting of a set of training examples. In supervised learning, each example is a pair consisting of an input object (typically a vector) and a desired output value (also called the supervisory signal). A supervised learning algorithm analyzes the training data and produces an inferred function, which can be used for mapping new examples. An optimal scenario will allow for the algorithm to correctly determine the class labels for unseen instances. This requires the learning algorithm to generalize from the training data to unseen situations in a reasonable way (see inductive bias).\n",
            "Model Response:learning, supervised, labeled, training, examples, input, instances, supervised,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading and Merging PEFT Model for Causal Language Modeling\n",
        "\n",
        "\n",
        "First, the PEFT model is loaded using the `AutoPeftModelForCausalLM.from_pretrained()` method with the following parameters:\n",
        "- `output_dir`: The directory where the PEFT model is saved.\n",
        "- `torch_dtype`: Specifies the Torch data type for computation, set to `torch.float16`.\n",
        "- `low_cpu_mem_usage`: Indicates whether to use low CPU memory usage mode, typically set to `True`.\n",
        "\n",
        "Next, the loaded PEFT model is merged with the base model using the `merge_and_unload()` method. This operation combines the PEFT-specific modifications with the base model's architecture and parameters.\n",
        "\n",
        "Finally, the merged model is saved using the `save_pretrained()` method with the following parameters:\n",
        "- `output_dir`: The directory where the merged model will be saved.\n",
        "- `safe_serialization`: Ensures safe serialization of the model.\n",
        "- `max_shard_size`: Specifies the maximum size of each file shard during saving, set to \"2GB\" to manage file size.\n",
        "\n",
        "This process results in a merged model containing the benefits of both the PEFT fine-tuning and the base model's architecture, suitable for efficient causal language modeling tasks.\n"
      ],
      "metadata": {
        "id": "agm9IKEt7nvK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load PEFT model on CPU for merging, if needed.\n",
        "model = AutoPeftModelForCausalLM.from_pretrained(\n",
        "    output_dir,\n",
        "    torch_dtype=torch.float16,\n",
        "    low_cpu_mem_usage=True,\n",
        ")\n",
        "\n",
        "# Merge LoRA and base model and save\n",
        "merged_model = model.merge_and_unload()\n",
        "merged_model.save_pretrained(\"full_model_dir\", safe_serialization=True, max_shard_size=\"2GB\")"
      ],
      "metadata": {
        "id": "TfxnM-jQdrmc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}